{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 7 - Berkeley STAT 157\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result: 0.23221   \n",
    "We tried Inception and Resnet152, and then found out that Inception_v3 produces a better result. In the Augmentation part, we change the size from 299 (originally used in the Inception paper) to 363.\n",
    "#### Parameters: Inception_v3   \n",
    "* Augmentation resize into 363 * 363   \n",
    "* num_epochs = 50   \n",
    "* learning rate: 1e-3   \n",
    "* weight decay: 1e-4\n",
    "* learning period and learning rate decay: 10, 0.1   \n",
    "* Drop out: 0.5\n",
    "![](result.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import d2l\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import model_zoo, nn\n",
    "from mxnet.gluon import data as gdata, loss as gloss, utils as gutils\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain and Organize the Data Sets\n",
    "\n",
    "The competition data is divided into a training set and testing set. The training set contains 10,222 images and the testing set contains 10,357 images. The images in both sets are in JPEG format. These images contain three RGB channels (color) and they have different heights and widths. There are 120 breeds of dogs in the training set, including Labradors, Poodles, Dachshunds, Samoyeds, Huskies, Chihuahuas, and Yorkshire Terriers.\n",
    "\n",
    "### Download the Data Set\n",
    "\n",
    "After logging in to Kaggle, we can click on the \"Data\" tab on the dog breed identification competition webpage shown in Figure 9.17 and download the training data set \"train.zip\", the testing data set \"test.zip\", and the training data set labels \"label.csv.zip\". After downloading the files, place them in the three paths below:\n",
    "\n",
    "* kaggle_dog/train.zip\n",
    "* kaggle_dog/test.zip\n",
    "* kaggle_dog/labels.csv.zip\n",
    "\n",
    "\n",
    "To make it easier to get started, we provide a small-scale sample of the data set mentioned above, \"train_valid_test_tiny.zip\". If you are going to use the full data set for the Kaggle competition, you will also need to change the `demo` variable below to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "# If you use the full data set downloaded for the Kaggle competition, \n",
    "# change the variable below to False.\n",
    "demo = False\n",
    "data_dir = '../kaggle_dog'\n",
    "if demo:\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "    gutils.download('https://github.com/d2l-ai/d2l-en/raw/master/data/kaggle_dog/train_valid_test_tiny.zip', \n",
    "                    data_dir)\n",
    "    zipfiles = ['train_valid_test_tiny.zip']\n",
    "else:\n",
    "    zipfiles = ['train.zip', 'test.zip', 'labels.csv.zip']\n",
    "for f in zipfiles:\n",
    "    with zipfile.ZipFile(data_dir + '/' + f, 'r') as z:\n",
    "        z.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize the Data Set\n",
    "\n",
    "Next, we define the `reorg_train_valid` function to segment the validation set from the original Kaggle competition training set.  The parameter `valid_ratio` in this function is the ratio of the number of examples of each dog breed in the validation set to the number of examples of the breed with the least examples (66) in the original training set. After organizing the data, images of the same breed will be placed in the same folder so that we can read them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label):\n",
    "    # The number of examples of the least represented breed in the training set.\n",
    "    min_n_train_per_label = (\n",
    "        collections.Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n",
    "    # The number of examples of each breed in the validation set.\n",
    "    n_valid_per_label = math.floor(min_n_train_per_label * valid_ratio)\n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = train_file.split('.')[0]\n",
    "        label = idx_label[idx]\n",
    "        d2l.mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            d2l.mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            d2l.mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'train', label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reorg_dog_data` function below is used to read the training data labels, segment the validation set, and organize the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio):\n",
    "    # Read the training data labels.\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        # Skip the file header line (column name).\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict(((idx, label) for idx, label in tokens))\n",
    "    reorg_train_valid(data_dir, train_dir, input_dir, valid_ratio, idx_label)\n",
    "    # Organize the training set.\n",
    "    d2l.mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using a small data set, we set the batch size to 1. During actual training and testing, we would use the entire Kaggle Competition data set and call the `reorg_dog_data` function to organize the data set. Likewise, we would need to set the `batch_size` to a larger integer, such as 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "if demo:\n",
    "    # Note: Here, we use a small data set and the batch size should be set\n",
    "    # smaller. When using the complete data set for the Kaggle competition, \n",
    "    # we can set the batch size to a larger integer.\n",
    "    input_dir, batch_size = 'train_valid_test_tiny', 1\n",
    "else:\n",
    "    label_file, train_dir, test_dir = 'labels.csv', 'train', 'test'\n",
    "    input_dir, batch_size, valid_ratio = 'train_valid_test', 128, 0.1\n",
    "    reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==========Image Augmentation  **: Size Change to 363========**\n",
    "\n",
    "The size of the images in this section are larger than the images in the previous section. Here are some more image augmentation operations that might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = gdata.vision.transforms.Compose([\n",
    "    # Randomly crop the image to obtain an image with an area of 0.08 to 1 \n",
    "    # of the original area and height to width ratio between 3/4 and 4/3.\n",
    "    # Then, scale the image to create a new image with a height and width \n",
    "    # of 224 pixels each.\n",
    "    gdata.vision.transforms.RandomResizedCrop(363, scale=(0.08, 1.0),\n",
    "                                              ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    gdata.vision.transforms.RandomFlipLeftRight(),\n",
    "    # Randomly change the brightness, contrast, and saturation.\n",
    "    gdata.vision.transforms.RandomColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                              saturation=0.4),\n",
    "    # Add random noise.\n",
    "    gdata.vision.transforms.RandomLighting(0.1),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    # Standardize each channel of the image.\n",
    "    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During testing, we only use definite image preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = gdata.vision.transforms.Compose([\n",
    "    gdata.vision.transforms.Resize(363),\n",
    "    # Crop a square of 224 by 224 from the center of the image.\n",
    "    gdata.vision.transforms.CenterCrop(363),\n",
    "    gdata.vision.transforms.ToTensor(),\n",
    "    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                      [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data Set\n",
    "\n",
    "As in the previous section, we can create an `ImageFolderDataset` instance to read the data set containing the original image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'train'), flag=1)\n",
    "valid_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'valid'), flag=1)\n",
    "train_valid_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'train_valid'), flag=1)\n",
    "test_ds = gdata.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, input_dir, 'test'), flag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a `DataLoader` instance, just like in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = gdata.DataLoader(train_ds.transform_first(transform_train),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "valid_iter = gdata.DataLoader(valid_ds.transform_first(transform_test),\n",
    "                              batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_iter = gdata.DataLoader(train_valid_ds.transform_first(\n",
    "    transform_train), batch_size, shuffle=True, last_batch='keep')\n",
    "test_iter = gdata.DataLoader(test_ds.transform_first(transform_test),\n",
    "                             batch_size, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model: Inception v3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "def get_net(ctx):\n",
    "    finetune_net = model_zoo.vision.inception_v3(pretrained=True)\n",
    "    # Define a new output network.\n",
    "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
    "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
    "    \n",
    "    #add dropout\n",
    "    finetune_net.output_new.add(nn.Dropout(0.5))\n",
    "    # There are 120 output categories.\n",
    "    finetune_net.output_new.add(nn.Dense(120))\n",
    "    # Initialize the output network.\n",
    "    finetune_net.output_new.initialize(init.Xavier(), ctx=ctx)\n",
    "    # Distribute the model parameters to the CPUs or GPUs used for computation.\n",
    "    finetune_net.collect_params().reset_ctx(ctx)\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the loss, we first use the member variable `features` to obtain the input of the pre-trained model's output layer, i.e. the extracted feature. Then, we use this feature as the input for our small custom output network and compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def evaluate_loss(data_iter, net, ctx):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.as_in_context(ctx)\n",
    "        output_features = net.features(X.as_in_context(ctx))\n",
    "        outputs = net.output_new(output_features)\n",
    "        l_sum += loss(outputs, y).sum().asscalar()\n",
    "        n += y.size\n",
    "    return l_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Functions\n",
    "\n",
    "We will select the model and tune hyper-parameters according to the model's performance on the validation set. The model training function `train` only trains the small custom output network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    # Only train the small custom output network.\n",
    "    trainer = gluon.Trainer(net.output_new.collect_params(), 'adam',\n",
    "                            {'learning_rate': lr, 'wd': wd})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, n, start = 0.0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for X, y in train_iter:\n",
    "            y = y.as_in_context(ctx)\n",
    "            output_features = net.features(X.as_in_context(ctx))\n",
    "            with autograd.record():\n",
    "                outputs = net.output_new(output_features)\n",
    "                l = loss(outputs, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += l.asscalar()\n",
    "            n += y.size\n",
    "        time_s = \"time %.2f sec\" % (time.time() - start)\n",
    "        if valid_iter is not None:\n",
    "            valid_loss = evaluate_loss(valid_iter, net, ctx)\n",
    "            epoch_s = (\"epoch %d, train loss %f, valid loss %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n, valid_loss))\n",
    "        else:\n",
    "            epoch_s = (\"epoch %d, train loss %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n))\n",
    "        print(epoch_s + time_s + ', lr ' + str(trainer.learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate the Model\n",
    "\n",
    "Now, we can train and validate the model. The following hyper-parameters can be tuned. For example, we can increase the number of epochs. Because `lr_period` and `lr_decay` are set to 10 and 0.1 respectively, the learning rate of the optimization algorithm will be multiplied by 0.1 after every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "ctx, num_epochs, lr, wd = d2l.try_gpu(), 50, 1e-3, 1e-4\n",
    "lr_period, lr_decay, net = 10, 0.1, get_net(ctx)\n",
    "net.hybridize()\n",
    "#train(net, train_iter, valid_iter, num_epochs, lr, wd, ctx, lr_period,\n",
    "#      lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the Testing Set and Submit Results on Kaggle\n",
    "\n",
    "After obtaining a satisfactory model design and hyper-parameters, we use all training data sets (including validation sets) to retrain the model and then classify the testing set. Note that predictions are made by the output network we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 2.982207, time 108.85 sec, lr 0.001\n",
      "epoch 2, train loss 1.452362, time 108.73 sec, lr 0.001\n",
      "epoch 3, train loss 1.210554, time 108.33 sec, lr 0.001\n",
      "epoch 4, train loss 1.135756, time 108.65 sec, lr 0.001\n",
      "epoch 5, train loss 1.064901, time 108.68 sec, lr 0.001\n",
      "epoch 6, train loss 1.006896, time 109.12 sec, lr 0.001\n",
      "epoch 7, train loss 1.013146, time 108.65 sec, lr 0.001\n",
      "epoch 8, train loss 0.975330, time 108.57 sec, lr 0.001\n",
      "epoch 9, train loss 0.948921, time 108.54 sec, lr 0.001\n",
      "epoch 10, train loss 0.951098, time 108.66 sec, lr 0.001\n",
      "epoch 11, train loss 0.907931, time 108.55 sec, lr 0.0001\n",
      "epoch 12, train loss 0.875904, time 108.75 sec, lr 0.0001\n",
      "epoch 13, train loss 0.867335, time 108.82 sec, lr 0.0001\n",
      "epoch 14, train loss 0.872187, time 108.37 sec, lr 0.0001\n",
      "epoch 15, train loss 0.847045, time 108.72 sec, lr 0.0001\n",
      "epoch 16, train loss 0.833357, time 108.67 sec, lr 0.0001\n",
      "epoch 17, train loss 0.843399, time 108.48 sec, lr 0.0001\n",
      "epoch 18, train loss 0.848024, time 108.66 sec, lr 0.0001\n",
      "epoch 19, train loss 0.839680, time 108.84 sec, lr 0.0001\n",
      "epoch 20, train loss 0.845426, time 108.47 sec, lr 0.0001\n",
      "epoch 21, train loss 0.867108, time 108.31 sec, lr 1e-05\n",
      "epoch 22, train loss 0.836042, time 108.70 sec, lr 1e-05\n",
      "epoch 23, train loss 0.829437, time 108.61 sec, lr 1e-05\n",
      "epoch 24, train loss 0.837398, time 108.16 sec, lr 1e-05\n",
      "epoch 25, train loss 0.836154, time 108.46 sec, lr 1e-05\n",
      "epoch 26, train loss 0.851179, time 108.76 sec, lr 1e-05\n",
      "epoch 27, train loss 0.868035, time 108.37 sec, lr 1e-05\n",
      "epoch 28, train loss 0.831694, time 108.52 sec, lr 1e-05\n",
      "epoch 29, train loss 0.841815, time 108.60 sec, lr 1e-05\n",
      "epoch 30, train loss 0.808178, time 108.50 sec, lr 1e-05\n",
      "epoch 31, train loss 0.827892, time 108.51 sec, lr 1.0000000000000002e-06\n",
      "epoch 32, train loss 0.849048, time 108.57 sec, lr 1.0000000000000002e-06\n",
      "epoch 33, train loss 0.832278, time 108.48 sec, lr 1.0000000000000002e-06\n",
      "epoch 34, train loss 0.845615, time 108.68 sec, lr 1.0000000000000002e-06\n",
      "epoch 35, train loss 0.844273, time 108.91 sec, lr 1.0000000000000002e-06\n",
      "epoch 36, train loss 0.845294, time 108.32 sec, lr 1.0000000000000002e-06\n",
      "epoch 37, train loss 0.832102, time 108.65 sec, lr 1.0000000000000002e-06\n",
      "epoch 38, train loss 0.837758, time 108.63 sec, lr 1.0000000000000002e-06\n",
      "epoch 39, train loss 0.835717, time 108.52 sec, lr 1.0000000000000002e-06\n",
      "epoch 40, train loss 0.847552, time 108.33 sec, lr 1.0000000000000002e-06\n",
      "epoch 41, train loss 0.849618, time 108.35 sec, lr 1.0000000000000002e-07\n",
      "epoch 42, train loss 0.815637, time 108.47 sec, lr 1.0000000000000002e-07\n",
      "epoch 43, train loss 0.831046, time 108.63 sec, lr 1.0000000000000002e-07\n",
      "epoch 44, train loss 0.831212, time 108.61 sec, lr 1.0000000000000002e-07\n",
      "epoch 45, train loss 0.846823, time 108.20 sec, lr 1.0000000000000002e-07\n",
      "epoch 46, train loss 0.846419, time 108.50 sec, lr 1.0000000000000002e-07\n",
      "epoch 47, train loss 0.839682, time 108.18 sec, lr 1.0000000000000002e-07\n",
      "epoch 48, train loss 0.849816, time 108.43 sec, lr 1.0000000000000002e-07\n",
      "epoch 49, train loss 0.821625, time 108.28 sec, lr 1.0000000000000002e-07\n",
      "epoch 50, train loss 0.841882, time 108.45 sec, lr 1.0000000000000002e-07\n"
     ]
    }
   ],
   "source": [
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, ctx, lr_period,\n",
    "      lr_decay)\n",
    "\n",
    "preds = []\n",
    "for data, label in test_iter:\n",
    "    output_features = net.features(data.as_in_context(ctx))\n",
    "    output = nd.softmax(net.output_new(output_features))\n",
    "    preds.extend(output.asnumpy())\n",
    "ids = sorted(os.listdir(os.path.join(data_dir, input_dir, 'test/unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
