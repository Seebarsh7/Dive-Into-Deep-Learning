{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 5 - Berkeley STAT 157\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T19:52:59.411749Z",
     "start_time": "2019-02-12T19:52:28.081528Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn, utils\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T19:52:59.411749Z",
     "start_time": "2019-02-12T19:52:28.081528Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_train = gdata.vision.FashionMNIST(train=True,transform=lambda data, label: (data.astype(np.float32), label))\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False,transform=lambda data, label: (data.astype(np.float32), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "1. Implement the logistic loss function $l(y,f) = -\\log(1 + \\exp(-y f))$ in Gluon.\n",
    "2. Plot its values and its derivative for $y = 1$ and $f \\in [-5, 5]$, using automatic differentiation in Gluon.\n",
    "3. Generate training and test datasets for a binary classification problem using Fashion-MNIST with class $1$ being a combination of `shirt` and `sweater` and class $-1$ being the combination of `sandal` and `sneaker` categories. \n",
    "4. Train a binary classifier of your choice (it can be linear or a simple MLP such as from a previous lecture) using half the data (i.e. $12,000$ observations mixed as abvove) and one using the full dataset (i.e. $24,000$ observations as arising from the 4 categories) and report its accuracy. \n",
    "\n",
    "Hint - you should encapsulate the training and reporting code in a callable function since you'll need it quite a bit in the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 1\n",
    "def loss(y,o):\n",
    "    ## add your loss function here\n",
    "    l = - nd.log(1 + nd.exp(- nd.dot(y, o)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "import matplotlib.pyplot as plt\n",
    "o = nd.arange(-5, 5, 0.01).reshape((1,1000))\n",
    "y = nd.array([1])\n",
    "l = loss(y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "onp = o.asnumpy()[0]\n",
    "plt.plot(onp,-l.asnumpy(), label = 'class of y = 1')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('logistic loss function')\n",
    "\n",
    "o.attach_grad()\n",
    "with autograd.record():\n",
    "    func = -loss(y,o)\n",
    "func.backward()\n",
    "plt.plot(onp,o.grad.asnumpy().reshape(1000,),label = 'grad')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(mnist_train, mnist_test,bias, total):\n",
    "    '''\n",
    "    biased ratio is for problem 3\n",
    "    total would be the total sample for each class.\n",
    "    '''\n",
    "    \n",
    "    X, y = mnist_train[:]\n",
    "    pullover =  np.where(y == 2)[0].tolist()\n",
    "    sneaker = np.where(y == 7)[0].tolist()\n",
    "    \n",
    "    shirt = np.where(y == 6)[0].tolist()\n",
    "    sandal = np.where(y == 5)[0].tolist()\n",
    "    \n",
    "    \n",
    "    class1_portion = round(bias * total)\n",
    "    class2_portion = round((1 - bias) * total)\n",
    "    \n",
    "    #for train dataset\n",
    "    #index_coat = random.sample(coat, class1_portion)\n",
    "    #index_shirt = random.sample(shirt, class1_portion)\n",
    "    #index_sandal = random.sample(sandal, class2_portion)\n",
    "    #index_sneaker = random.sample(sneaker, class2_portion)\n",
    "    \n",
    "    index_pullover = pullover[0:class1_portion] # 600\n",
    "    index_sneaker = sneaker[0:class1_portion] # 600\n",
    "    \n",
    "    index_shirt = shirt[0:class2_portion] # 5400\n",
    "    index_sandal = sandal[0:class2_portion] #5400\n",
    "\n",
    "    \n",
    "    train_pullover = X[index_pullover]\n",
    "    train_sneaker = X[index_sneaker]\n",
    "    \n",
    "    train_shirt = X[index_shirt]\n",
    "    train_sandal = X[index_sandal]\n",
    "    \n",
    "     \n",
    "    train_feature = nd.concat(train_pullover, train_sneaker, train_shirt, train_sandal, dim=0)\n",
    "    #train_feature = nd.concat(train_pullover, train_shirt, train_sneaker, train_sandal, dim=0)\n",
    "    \n",
    "    label1 = nd.ones((1, total)).astype(np.float32)\n",
    "    label2 = nd.zeros((1, total)).astype(np.float32)\n",
    "    \n",
    "    train_labels = nd.concat(label1, label2, dim=1).reshape(shape=(-1,))\n",
    "    train_data = gdata.dataset.ArrayDataset(train_feature, train_labels)\n",
    "    \n",
    "    #for test dataset    \n",
    "    A, b = mnist_test[:]\n",
    "    indices_1 = np.where(np.logical_or(b == 2, b == 7))[0].tolist() #clothes flipped\n",
    "    indices_2 = np.where(np.logical_or(b == 6, b == 5))[0].tolist() #shoes\n",
    "    \n",
    "    #indices_1 = np.where(np.logical_or(b == 2, b == 6))[0].tolist() #clothes\n",
    "    #indices_2 = np.where(np.logical_or(b == 7, b == 5))[0].tolist() #shoes\n",
    "    class1 = A[indices_1] #class1\n",
    "    class0 = A[indices_2] #class-1\n",
    "    test_feature = nd.concat(class1, class0,dim = 0)\n",
    "    label1 = nd.ones((1, 2000)).astype(np.float32)\n",
    "    label0 = nd.zeros((1,2000)).astype(np.float32)\n",
    "    test_label = nd.concat(label1,label0,dim = 1).reshape(shape=(-1,))\n",
    "    test_data = gdata.dataset.ArrayDataset(test_feature, test_label)    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l,random\n",
    "train_all, test_all = generate_data(mnist_train, mnist_test,0.5, 12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train, test, batch_size, lr, num_epochs):\n",
    "    train_iter = gdata.DataLoader(train, batch_size,shuffle = True)\n",
    "    test_iter = gdata.DataLoader(test, batch_size,shuffle = True)\n",
    "\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(2))\n",
    "    net.initialize(init.Normal(sigma=0.01))\n",
    "\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 10825.1328, train acc 0.645, test acc 0.762\n",
      "epoch 2, loss 5235.8789, train acc 0.758, test acc 0.858\n",
      "epoch 3, loss 4827.5645, train acc 0.778, test acc 0.869\n",
      "epoch 4, loss 4314.0024, train acc 0.793, test acc 0.848\n",
      "epoch 5, loss 4242.1430, train acc 0.793, test acc 0.869\n",
      "epoch 6, loss 3631.4309, train acc 0.815, test acc 0.873\n",
      "epoch 7, loss 4381.1091, train acc 0.792, test acc 0.875\n",
      "epoch 8, loss 3458.9050, train acc 0.818, test acc 0.845\n",
      "epoch 9, loss 3706.5656, train acc 0.807, test acc 0.881\n",
      "epoch 10, loss 3695.6305, train acc 0.812, test acc 0.874\n"
     ]
    }
   ],
   "source": [
    "model(train_all, test_all, 256, 0.02, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train half sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_half, test_half = generate_data(mnist_train, mnist_test,0.5, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 6761.6128, train acc 0.585, test acc 0.659\n",
      "epoch 2, loss 3832.0859, train acc 0.689, test acc 0.612\n",
      "epoch 3, loss 2934.9573, train acc 0.740, test acc 0.815\n",
      "epoch 4, loss 2744.7616, train acc 0.759, test acc 0.813\n",
      "epoch 5, loss 2139.4865, train acc 0.795, test acc 0.852\n",
      "epoch 6, loss 2668.8411, train acc 0.762, test acc 0.870\n",
      "epoch 7, loss 2620.7927, train acc 0.768, test acc 0.846\n",
      "epoch 8, loss 2140.0285, train acc 0.804, test acc 0.806\n",
      "epoch 9, loss 2364.9301, train acc 0.778, test acc 0.871\n",
      "epoch 10, loss 2151.9523, train acc 0.797, test acc 0.874\n"
     ]
    }
   ],
   "source": [
    "model(train_half, test_half, 256, 0.01, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Covariate Shift\n",
    "\n",
    "Your goal is to introduce covariate shit in the data and observe the accuracy. For this, compose a dataset of $12,000$ observations, given by a mixture of `shirt` and `sweater` and of `sandal` and `sneaker` respectively, where you use a fraction $\\lambda \\in \\{0.05, 0.1, 0.2, \\ldots 0.8, 0.9, 0.95\\}$ of one and a fraction of $1-\\lambda$ of  the other datasets respectively. For instance, you might pick for $\\lambda = 0.1$ a total of $600$ `shirt` and $600$ `sweater` images and likewise $5,400$ `sandal` and $5,400$ `sneaker` photos, yielding a total of $12,000$ images for training. Note that the test set remains unbiased, composed of $2,000$ photos for the `shirt` + `sweater` category and of the `sandal` + `sneaker` category each.\n",
    "\n",
    "1. Generate training sets that are appropriately biased. You should have 11 datasets.\n",
    "2. Train a binary classifier using this and report the test set accuracy on the unbiased test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias is 0.05\n",
      "epoch 1, loss 941.2027, train acc 0.874, test acc 0.570\n",
      "epoch 2, loss 722.3783, train acc 0.901, test acc 0.590\n",
      "epoch 3, loss 673.3000, train acc 0.903, test acc 0.504\n",
      "epoch 4, loss 719.7617, train acc 0.911, test acc 0.616\n",
      "epoch 5, loss 502.3603, train acc 0.908, test acc 0.660\n",
      "Bias is 0.10\n",
      "epoch 1, loss 2063.1038, train acc 0.791, test acc 0.583\n",
      "epoch 2, loss 1429.2275, train acc 0.840, test acc 0.693\n",
      "epoch 3, loss 1391.8873, train acc 0.842, test acc 0.627\n",
      "epoch 4, loss 1431.7786, train acc 0.844, test acc 0.588\n",
      "epoch 5, loss 1480.7845, train acc 0.845, test acc 0.644\n",
      "Bias is 0.20\n",
      "epoch 1, loss 3916.1632, train acc 0.686, test acc 0.588\n",
      "epoch 2, loss 3120.7885, train acc 0.734, test acc 0.702\n",
      "epoch 3, loss 2814.6490, train acc 0.751, test acc 0.699\n",
      "epoch 4, loss 2980.3434, train acc 0.751, test acc 0.723\n",
      "epoch 5, loss 2866.8748, train acc 0.763, test acc 0.697\n",
      "Bias is 0.30\n",
      "epoch 1, loss 5222.9813, train acc 0.628, test acc 0.641\n",
      "epoch 2, loss 4453.8252, train acc 0.688, test acc 0.740\n",
      "epoch 3, loss 4118.0642, train acc 0.703, test acc 0.654\n",
      "epoch 4, loss 3794.8105, train acc 0.725, test acc 0.769\n",
      "epoch 5, loss 3784.6700, train acc 0.721, test acc 0.712\n",
      "Bias is 0.40\n",
      "epoch 1, loss 6504.6285, train acc 0.606, test acc 0.799\n",
      "epoch 2, loss 4830.9714, train acc 0.679, test acc 0.856\n",
      "epoch 3, loss 3790.8177, train acc 0.716, test acc 0.730\n",
      "epoch 4, loss 3782.4454, train acc 0.720, test acc 0.775\n",
      "epoch 5, loss 3406.6403, train acc 0.738, test acc 0.772\n",
      "Bias is 0.50\n",
      "epoch 1, loss 5734.3032, train acc 0.635, test acc 0.783\n",
      "epoch 2, loss 2818.3283, train acc 0.758, test acc 0.867\n",
      "epoch 3, loss 2889.3432, train acc 0.756, test acc 0.826\n",
      "epoch 4, loss 2229.7475, train acc 0.796, test acc 0.868\n",
      "epoch 5, loss 2294.7648, train acc 0.797, test acc 0.875\n",
      "Bias is 0.60\n",
      "epoch 1, loss 7353.4950, train acc 0.582, test acc 0.806\n",
      "epoch 2, loss 4836.4141, train acc 0.656, test acc 0.648\n",
      "epoch 3, loss 4247.8479, train acc 0.667, test acc 0.766\n",
      "epoch 4, loss 3925.1640, train acc 0.684, test acc 0.852\n",
      "epoch 5, loss 3644.2572, train acc 0.699, test acc 0.816\n",
      "Bias is 0.70\n",
      "epoch 1, loss 6158.6528, train acc 0.612, test acc 0.648\n",
      "epoch 2, loss 4677.0312, train acc 0.656, test acc 0.659\n",
      "epoch 3, loss 4043.7325, train acc 0.676, test acc 0.669\n",
      "epoch 4, loss 3890.6088, train acc 0.687, test acc 0.674\n",
      "epoch 5, loss 3649.7682, train acc 0.690, test acc 0.669\n",
      "Bias is 0.80\n",
      "epoch 1, loss 4606.4829, train acc 0.695, test acc 0.476\n",
      "epoch 2, loss 3474.9436, train acc 0.720, test acc 0.593\n",
      "epoch 3, loss 3041.3735, train acc 0.739, test acc 0.660\n",
      "epoch 4, loss 2966.8490, train acc 0.740, test acc 0.603\n",
      "epoch 5, loss 2666.7816, train acc 0.746, test acc 0.611\n",
      "Bias is 0.90\n",
      "epoch 1, loss 2576.5770, train acc 0.813, test acc 0.490\n",
      "epoch 2, loss 1745.7416, train acc 0.841, test acc 0.618\n",
      "epoch 3, loss 1819.3211, train acc 0.840, test acc 0.514\n",
      "epoch 4, loss 1656.4625, train acc 0.840, test acc 0.559\n",
      "epoch 5, loss 1389.0120, train acc 0.852, test acc 0.651\n",
      "Bias is 0.95\n",
      "epoch 1, loss 1598.6116, train acc 0.890, test acc 0.591\n",
      "epoch 2, loss 1106.3671, train acc 0.912, test acc 0.607\n",
      "epoch 3, loss 841.6675, train acc 0.911, test acc 0.543\n",
      "epoch 4, loss 790.9256, train acc 0.907, test acc 0.502\n",
      "epoch 5, loss 708.6425, train acc 0.921, test acc 0.562\n"
     ]
    }
   ],
   "source": [
    "for i in lam:\n",
    "    train_shift, test_shift = generate_data(mnist_train, mnist_test,i, 6000)\n",
    "    print('Bias is %.2f' %i)\n",
    "    model(train_shift, test_shift, 128, 0.01, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Covariate Shift Correction\n",
    "\n",
    "Having observed that covariate shift can be harmful, let's try fixing it. For this we first need to compute the appropriate propensity scores $\\frac{dp(x)}{dq(x)}$. For this purpose pick a biased dataset, let's say with $\\lambda = 0.1$ and try to fix the covariate shift.\n",
    "\n",
    "1. When training a logistic regression binary classifier to fix covariate shift, we assumed so far that both sets are of equal size. Show that re-weighting data in training and test set appropriately can help address the issue when both datasets have different size. What is the weighting?\n",
    "2. Train a binary classifier (using logistic regression) distinguishing between the biased training set and the unbiased test set. Note - you need to weigh the data. \n",
    "3. Use the scores to compute weights on the training set. Do they match the weight arising from the biasing distribution $\\lambda$? \n",
    "4. Train a binary classifier of the covariate shifted problem using the weights obtained previously and report the accuracy. Note - you will need to modify the training loop slightly such that you can compute the gradient of a weighted sum of losses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "1. To minimize the loss function when the data come from unbiased dataset, which has the distribution p(x) (here is the testing set): \n",
    "$$\\min_w\\int{dxp(x)}\\int{dyp(y|x)l(f(x,w),y)}$$\n",
    "where p(x) is the 'correct' distribution and q(x) is the 'wrong' one.   \n",
    "While the empirical form could be descirbed as:\n",
    "$$\\min_x\\frac{1}{n}\\sum_{i}(l(x_i,y_i),f(x_i))$$\n",
    "But when the data come from biased dataset(here is the training set, which has the distribution q(x), the formula becomes:\n",
    "$$\\min_w\\int{dxq(x)}\\int{dyp(y|x)l(f(x,w),y)}$$\n",
    "So re-weighting data to adjust the q(x) back to p(x) would help the function become unbiased. Therefore, the weight when training the binary classification should be:\n",
    "$$\\beta(x) = \\frac{p(x)}{q(x)} $$\n",
    "The joint probability distribution is (label data from train as -1, from test as 1:\n",
    "$$r(x,y) = \\frac{N_{test}}{N}p(x)\\delta(y,1) + \\frac{N_{train}}{N}q(x)\\delta(y,-1)$$\n",
    "$$r(y = 1|x) = \\frac{r(x,y=1)}{r(x)} = \\frac{r(x,y=1)}{r(x|y=1) + r(x|y=-1)} = $$   \n",
    "$$\\frac{r(x,y=1)}{r(x, y=1) + r(x, y=-1)} = \\frac{N_{test}p(x)}{N_{test}p(x)+N_{train}q(x)}$$\n",
    "$$r(y = -1|x) = \\frac{r(x,y=-1)}{r(x)} = \\frac{r(x,y=-1)}{r(x|y=-1) + r(x|y=1)} = $$    \n",
    "$$\\frac{r(x,y=-1)}{r(x, y=1) + r(x, y=-1)} = \\frac{N_{train}q(x)}{N_{test}p(x)+N_{train}q(x)}$$\n",
    "$$\\beta(x) = \\frac{p(x)}{q(x)} = \\frac{r(y = 1|x)}{r(y = -1|x)}*\\frac{N_{train}}{N_{test}} = \\frac{N_{train}}{N_{test}} exp(f(x))$$\n",
    "Therefore, the weight is: $$\\frac{N_{train}}{N_{test}}exp(f(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give training set label 0 and testing set label 1. And 11000 for training, 5000 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(ratio, batch_size):\n",
    "    mnist_train = gdata.vision.FashionMNIST(train=True,transform=lambda data, label: (data.astype(np.float32)/255.0, label))\n",
    "    mnist_test = gdata.vision.FashionMNIST(train=False,transform=lambda data, label: (data.astype(np.float32)/255.0, label))\n",
    "    bias_train, bias_test = generate_data(mnist_train, mnist_test,ratio, 6000)\n",
    "    #train's label should be 1 and test's label should be 0\n",
    "    f_train, l_train = bias_train[:]\n",
    "    f_test, l_test = bias_test[:]\n",
    "    \n",
    "    l_train = nd.zeros((1, 12000)).astype(np.float32) \n",
    "    l_test = nd.ones((1,4000)).astype(np.float32)\n",
    "    \n",
    "    trainLabel = nd.concat(l_train[:,:8000],l_test[:,:3000],dim = 1).reshape(shape=(-1,))\n",
    "    testLabel = nd.concat(l_train[:,8000:],l_test[:,3000:],dim = 1).reshape(shape=(-1,))\n",
    "    trainFeature = nd.concat(f_train[:8000], f_test[:3000], dim = 0)\n",
    "    testFeature = nd.concat(f_train[8000:], f_test[3000:], dim = 0)\n",
    "\n",
    "    trainDataset = gdata.dataset.ArrayDataset(trainFeature, trainLabel) \n",
    "    testDataset = gdata.dataset.ArrayDataset(testFeature, testLabel) \n",
    "    \n",
    "    train = gluon.data.DataLoader(trainDataset, batch_size=batch_size, shuffle=True)\n",
    "    test = gluon.data.DataLoader(testDataset, batch_size=batch_size, shuffle=True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not very high accuracy for binary classification\n",
    "* The logic behind the covariate shift.   \n",
    "    1. There is only one output here and it would be class 1. I have already generating a dataset with data from training set labeled as 1, and data from testing set labeled as 0. The weight here should be 5 adding to the loss function.   \n",
    "    2. find the function $f$, which is trained by the Gluon net (a linear function) using the binary classification implemented in problem 2.   \n",
    "    3. Weigh training data using $$\\beta_i = min(\\exp(f(x_i)),c)$$    \n",
    "    and c would be a ceiling preventing the exponential function to be NAN   \n",
    "    4. Using the weights $\\beta_i$ for training on X with labels Y, which is problem 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Logistic function and Loss function    \n",
    "reference from Gluon tutorial: Binary Classification with Logistic Regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1. / (1. + nd.exp(-z))\n",
    "\n",
    "def log_loss(output, y):\n",
    "    yhat = logistic(output)\n",
    "    yhat = yhat.reshape(shape=y.shape)\n",
    "    return  - nd.nansum(3. * y * nd.log(yhat) + 1.*(1-y) * nd.log(1-yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, train_data, net, trainer, batch_size, ratio):\n",
    "    for e in range(epochs):\n",
    "        cumulative_loss = 0\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = log_loss(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            cumulative_loss += nd.sum(loss).asscalar()\n",
    "        print(\"Epoch %s, loss: %s\" % (e, cumulative_loss ))\n",
    "\n",
    "\n",
    "def test_model(test_data):\n",
    "    num_correct = 0.0\n",
    "    num_total = 0\n",
    "    for i, (data, label) in enumerate(test_data):\n",
    "        num_total += len(label)\n",
    "        output = net(data)\n",
    "        prediction = ((nd.sign(output).reshape(shape=label.shape) + 1) / 2)\n",
    "        num_correct += nd.sum(prediction == label)\n",
    "    print(\"Accuracy: %0.3f (%s/%s)\" % (num_correct.asscalar()/num_total, num_correct.asscalar(), num_total))\n",
    "\n",
    "def train_and_test(num_epochs, train_data, test_data, net, trainer, batch_size, ratio):\n",
    "    train_model(num_epochs, train_data, net, trainer, batch_size, ratio)\n",
    "    test_model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 6473.40641784668\n",
      "Epoch 1, loss: 6215.412225723267\n",
      "Epoch 2, loss: 6132.110326766968\n",
      "Epoch 3, loss: 6110.929149627686\n",
      "Epoch 4, loss: 6177.142705917358\n",
      "Epoch 5, loss: 5999.846092224121\n",
      "Epoch 6, loss: 6117.480098724365\n",
      "Epoch 7, loss: 6006.058481216431\n",
      "Epoch 8, loss: 6031.719369888306\n",
      "Epoch 9, loss: 6040.584384918213\n",
      "Epoch 10, loss: 6006.04602432251\n",
      "Epoch 11, loss: 6074.275054931641\n",
      "Epoch 12, loss: 6071.352899551392\n",
      "Epoch 13, loss: 6009.529308319092\n",
      "Epoch 14, loss: 6051.378656387329\n",
      "Epoch 15, loss: 5920.292963027954\n",
      "Epoch 16, loss: 5947.89112663269\n",
      "Epoch 17, loss: 6008.107885360718\n",
      "Epoch 18, loss: 5951.04513168335\n",
      "Epoch 19, loss: 6007.231288909912\n",
      "Epoch 20, loss: 5935.430620193481\n",
      "Epoch 21, loss: 5966.720666885376\n",
      "Epoch 22, loss: 5978.050519943237\n",
      "Epoch 23, loss: 5958.027393341064\n",
      "Epoch 24, loss: 5995.020748138428\n",
      "Epoch 25, loss: 5891.928075790405\n",
      "Epoch 26, loss: 5888.551877975464\n",
      "Epoch 27, loss: 5889.394569396973\n",
      "Epoch 28, loss: 5986.5500564575195\n",
      "Epoch 29, loss: 5919.4931354522705\n",
      "Epoch 30, loss: 5932.525381088257\n",
      "Epoch 31, loss: 5928.498115539551\n",
      "Epoch 32, loss: 5904.316030502319\n",
      "Epoch 33, loss: 5921.581356048584\n",
      "Epoch 34, loss: 5979.360321044922\n",
      "Epoch 35, loss: 6018.547925949097\n",
      "Epoch 36, loss: 5847.301422119141\n",
      "Epoch 37, loss: 5872.218954086304\n",
      "Epoch 38, loss: 5914.643138885498\n",
      "Epoch 39, loss: 5915.912443161011\n",
      "Epoch 40, loss: 5894.584888458252\n",
      "Epoch 41, loss: 5957.983530044556\n",
      "Epoch 42, loss: 5907.7888469696045\n",
      "Epoch 43, loss: 5940.209215164185\n",
      "Epoch 44, loss: 5912.14271736145\n",
      "Epoch 45, loss: 5884.8051109313965\n",
      "Epoch 46, loss: 5855.341190338135\n",
      "Epoch 47, loss: 5968.199106216431\n",
      "Epoch 48, loss: 5840.906837463379\n",
      "Epoch 49, loss: 5900.898103713989\n",
      "Accuracy: 0.754 (3772.0/5000)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "total_per_label = 6000\n",
    "ratio = 0.2\n",
    "num_epochs = 50\n",
    "net = nn.Dense(1)\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.01))\n",
    "train_data, test_data = generator(ratio, batch_size)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.08})\n",
    "\n",
    "train_and_test(num_epochs, train_data, test_data, net, trainer, batch_size, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.3    \n",
    "The weight is similar  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.01]\n",
       " [10.  ]\n",
       " [10.  ]\n",
       " ...\n",
       " [10.  ]\n",
       " [10.  ]\n",
       " [10.  ]]\n",
       "<NDArray 12000x1 @cpu(0)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biasTrain, biasTest = generate_data(mnist_train, mnist_test,0.2, 6000)\n",
    "X, y = biasTrain[:]\n",
    "weight = nd.clip(nd.exp(net(X)), 0.01, 10)\n",
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1. / (1. + nd.exp(-z))\n",
    "\n",
    "#def log_loss(output, y):\n",
    "    #yhat = logistic(output)\n",
    "    #yhat = yhat.reshape(shape=y.shape)\n",
    "    #return  - nd.nansum(y * nd.log(yhat) + (1-y) * nd.log(1-yhat))\n",
    "    \n",
    "log_loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "def train_model2(epochs, train_data, net, trainer, batch_size, weight):\n",
    "    for e in range(epochs):\n",
    "        cumulative_loss = 0\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = log_loss(output, label) * weight[i]\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            cumulative_loss += nd.sum(loss).asscalar()\n",
    "        print(\"Epoch %s, loss: %s\" % (e, cumulative_loss ))\n",
    "\n",
    "\n",
    "def test_model2(test_data):\n",
    "    num_correct = 0.0\n",
    "    num_total = 0\n",
    "    for i, (data, label) in enumerate(test_data):\n",
    "        num_total += len(label)\n",
    "        output = net(data)\n",
    "        prediction = ((nd.sign(output).reshape(shape=label.shape) + 1) / 2)\n",
    "        num_correct += nd.sum(prediction == label)\n",
    "    print(\"Accuracy: %0.3f (%s/%s)\" % (num_correct.asscalar()/num_total, num_correct.asscalar(), num_total))\n",
    "    \n",
    "def train_and_test2(num_epochs, train_data, test_data, net, trainer, batch_size,weight):\n",
    "    train_model2(num_epochs, train_data, net, trainer, batch_size, weight)\n",
    "    test_model2(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1766236340.3711119\n",
      "Epoch 1, loss: 1287476280.6531982\n",
      "Epoch 2, loss: 1281698326.2346191\n",
      "Epoch 3, loss: 955634900.8786621\n",
      "Epoch 4, loss: 1236339872.6142578\n",
      "Epoch 5, loss: 1223799088.8310547\n",
      "Epoch 6, loss: 1542249789.7416992\n",
      "Epoch 7, loss: 1547246502.5986328\n",
      "Epoch 8, loss: 932278630.1938477\n",
      "Epoch 9, loss: 1301414251.2641602\n",
      "Epoch 10, loss: 961563285.1958008\n",
      "Epoch 11, loss: 1076797662.0896606\n",
      "Epoch 12, loss: 1174117646.4117432\n",
      "Epoch 13, loss: 1106483277.7299805\n",
      "Epoch 14, loss: 975072617.9836426\n",
      "Epoch 15, loss: 1386321930.1940918\n",
      "Epoch 16, loss: 1173450111.755371\n",
      "Epoch 17, loss: 1191400851.7270508\n",
      "Epoch 18, loss: 1254733979.2612305\n",
      "Epoch 19, loss: 1105659771.340088\n",
      "Epoch 20, loss: 1079857344.9448242\n",
      "Epoch 21, loss: 970116924.9335938\n",
      "Epoch 22, loss: 1211784464.3154297\n",
      "Epoch 23, loss: 1156371817.6904297\n",
      "Epoch 24, loss: 965434056.7720947\n",
      "Epoch 25, loss: 919804337.9458771\n",
      "Epoch 26, loss: 1211614108.5808105\n",
      "Epoch 27, loss: 1087033792.0927734\n",
      "Epoch 28, loss: 1044784565.9194336\n",
      "Epoch 29, loss: 1073001230.5681152\n",
      "Epoch 30, loss: 1100649894.102539\n",
      "Epoch 31, loss: 1073796600.9320068\n",
      "Epoch 32, loss: 887513092.3607178\n",
      "Epoch 33, loss: 862032273.4171143\n",
      "Epoch 34, loss: 956796112.815918\n",
      "Epoch 35, loss: 1275538219.0078125\n",
      "Epoch 36, loss: 1346364200.437378\n",
      "Epoch 37, loss: 1062693307.6567383\n",
      "Epoch 38, loss: 1011653963.8918457\n",
      "Epoch 39, loss: 1086954316.0854492\n",
      "Epoch 40, loss: 1148770797.1411438\n",
      "Epoch 41, loss: 1157969427.9824219\n",
      "Epoch 42, loss: 758833315.7260742\n",
      "Epoch 43, loss: 987003372.6928711\n",
      "Epoch 44, loss: 962010399.5706787\n",
      "Epoch 45, loss: 861622949.4667969\n",
      "Epoch 46, loss: 1023120091.3591309\n",
      "Epoch 47, loss: 1134497959.3950195\n",
      "Epoch 48, loss: 833236970.2109375\n",
      "Epoch 49, loss: 1181769685.12323\n",
      "Accuracy: 0.748 (2990.0/4000)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "total_per_label = 6000\n",
    "ratio = 0.2\n",
    "epochs = 50\n",
    "net = nn.Dense(1)\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.01))\n",
    "train_half, test_half = generate_data(mnist_train, mnist_test,0.2, 6000)\n",
    "\n",
    "Train = gluon.data.DataLoader(train_half, batch_size=batch_size, shuffle=True)\n",
    "Test = gluon.data.DataLoader(test_half, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})\n",
    "\n",
    "train_and_test2(epochs, Train, Test, net, trainer, batch_size, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the un-reweight one, the accuracy kind of improves. However, the binary classification for classifying unbiased and biased dataset remained at an accuracy of 0.80, which limited the performance of covariate shifting (even if I run it for 2000 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 24))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import ndarray as nd\n",
    "def to_onehot(X, size):\n",
    "    return [nd.one_hot(x, size) for x in X.T]\n",
    "\n",
    "X = nd.arange(10).reshape((2, 5))\n",
    "inputs = to_onehot(X, 24)\n",
    "len(inputs), inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
